{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 아래 코드 230721 정유라 추가함\n",
    "import gc\n",
    "from skimage.color import lab2lch, rgb2lab\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.morphology import disk\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape): # rle_디코딩\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(image, subimage_size=256):\n",
    "\n",
    "    subimages = []\n",
    "    for i in range(0, image.shape[0], subimage_size):\n",
    "        for j in range(0, image.shape[1], subimage_size):\n",
    "            subimage = image[i:i+subimage_size, j:j+subimage_size]\n",
    "            subimages.append(subimage)\n",
    "    return subimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train_img/TRAIN_0000.png</td>\n",
       "      <td>9576 7 10590 17 11614 17 12638 17 13662 17 146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train_img/TRAIN_0001.png</td>\n",
       "      <td>208402 1 209425 6 210449 10 211473 14 212497 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train_img/TRAIN_0002.png</td>\n",
       "      <td>855 34 15654 9 16678 9 16742 8 17702 9 17766 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train_img/TRAIN_0003.png</td>\n",
       "      <td>362 6 745 15 798 22 900 25 1385 8 1828 16 1924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train_img/TRAIN_0004.png</td>\n",
       "      <td>34 27 1058 27 2082 27 3105 27 4129 27 5153 27 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       img_id                    img_path  \\\n",
       "0  TRAIN_0000  ./train_img/TRAIN_0000.png   \n",
       "1  TRAIN_0001  ./train_img/TRAIN_0001.png   \n",
       "2  TRAIN_0002  ./train_img/TRAIN_0002.png   \n",
       "3  TRAIN_0003  ./train_img/TRAIN_0003.png   \n",
       "4  TRAIN_0004  ./train_img/TRAIN_0004.png   \n",
       "\n",
       "                                            mask_rle  \n",
       "0  9576 7 10590 17 11614 17 12638 17 13662 17 146...  \n",
       "1  208402 1 209425 6 210449 10 211473 14 212497 1...  \n",
       "2  855 34 15654 9 16678 9 16742 8 17702 9 17766 9...  \n",
       "3  362 6 745 15 798 22 900 25 1385 8 1828 16 1924...  \n",
       "4  34 27 1058 27 2082 27 3105 27 4129 27 5153 27 ...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "dat = pd.read_csv(os.path.join(current_dir, 'train.csv'))\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# save_img = './cut_img' # 자른 이미지 저장\n",
    "# create_directory(save_img)\n",
    "\n",
    "# save_msk = './cut_msk' # 자른 마스크 저장\n",
    "# create_directory(save_msk)\n",
    "\n",
    "save_img_2 = './1_cut_img'  # 자른 이미지 저장\n",
    "create_directory(save_img_2)\n",
    "\n",
    "save_msk_2 = './1_cut_msk'  # 자른 마스크 저장\n",
    "create_directory(save_msk_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여기서 저장할 이미지 개수를 조절해요. \n",
    "\n",
    "# len(dat) = 7140\n",
    "lens = len(dat) -7139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_and_plot(images, bin_edges):\n",
    "#     pixel_counts = []\n",
    "\n",
    "#     # Iterate through all images in the folder\n",
    "#     for image in images:\n",
    "#         x, y = image.shape\n",
    "#         building_pixels = (image > 0).astype(np.uint8) # <-- 바꾼 부분\n",
    "#         # Count building pixels and add to the list\n",
    "#         building_pixels = np.sum(image == 1)\n",
    "#         pixel_counts.append(building_pixels/(x*y)*100)\n",
    "#         # print(f'{building_pixels} / {x*y}')\n",
    "    \n",
    "#     # Sorting the pixel counts\n",
    "#     pixel_counts.sort()\n",
    "\n",
    "#     # Preparing bins for the histogram\n",
    "#     bin_labels = [f'{bin_edges[i]} - {bin_edges[i+1]}' for i in range(len(bin_edges)-1)]\n",
    "\n",
    "#     # Plotting the histogram\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     plt.hist(pixel_counts, bins=bin_edges, edgecolor='black', alpha=0.7)\n",
    "#     plt.title(\"Distribution of Building Pixels Across Images\")\n",
    "#     plt.xlabel(\"Number of Building Pixels\")\n",
    "#     plt.ylabel(\"Number of Images\")\n",
    "#     plt.xticks(bin_edges, fontsize=1)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Returning the counts as well\n",
    "#     return pixel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지, 마스크 페어로 저장 복사본\n",
    "\n",
    "for i in range(0, lens):\n",
    "    \n",
    "    # 이미지 불러오기\n",
    "    image = cv2.imread(dat.iloc[i,1][2:])\n",
    "    # 마스크 불러오기\n",
    "    mask = rle_decode(dat.iloc[i, 2], (1024, 1024))\n",
    "\n",
    "    # 이미지를 256*256 사이즈의 서브 이미지로 분할\n",
    "    sub_images = cut_image(image, 256)\n",
    "    # 마스크를 256x256 사이즈의 서브 마스크로 분할\n",
    "    sub_masks = cut_image(mask, 256)\n",
    "\n",
    "    # 분할된 마스크를 png로 저장\n",
    "    for idx, (submask, subimage) in enumerate(zip(sub_masks, sub_images)):\n",
    " \n",
    "        white_pixels = np.count_nonzero(submask > 0)  # 이진영상에서 건물 유무 확인\n",
    "    \n",
    "        \n",
    "        if white_pixels > 0: # 건물 영역인 픽셀의 개수가 0개 초과인 경우\n",
    "            \n",
    "            cv2.imwrite(f'{save_img_2}/1_TRAIN_{dat.iloc[i,0][6:]}_{str(idx+1).zfill(2)}.png', subimage)\n",
    "            cv2.imwrite(f'{save_msk_2}/1_TRAIN_{dat.iloc[i,0][6:]}_{str(idx+1).zfill(2)}.png', submask) # 파일명 ex) cut_msk/1_TRAIN_0000_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None, infer=False):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.image_filenames = os.listdir(image_folder)\n",
    "        self.mask_filenames = os.listdir(mask_folder)\n",
    "        self.infer = infer\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_folder, self.image_filenames[index])\n",
    "        mask_path = os.path.join(self.mask_folder, self.mask_filenames[index])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "#     transforms.RandomHorizontalFlip(),  # 랜덤 수평 뒤집기\n",
    "#     transforms.RandomVerticalFlip(),  # 랜덤 수직 뒤집기\n",
    "#     transforms.RandomRotation(30),  # 랜덤 회전 (최대 30도)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 랜덤 컬러 조정\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Define the U-Net model with ResNet34 backbone\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet34',        # Use ResNet34 as the encoder backbone\n",
    "    encoder_weights='imagenet',     # Use pretrained weights on ImageNet\n",
    "    in_channels=3,                  # Input channels, adjust according to your data\n",
    "    classes=1,                      # Number of output classes, adjust according to your task\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define other training parameters (adjust as needed)\n",
    "batch_size = 16\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "dataset = ImageDataset( image_folder = './1_cut_img', mask_folder = './1_cut_msk', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7218149900436401\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for this epoch\n",
    "    train_loss = running_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, criterion, optimizer, train_dataloader, device)\n",
    "    torch.save(model.state_dict(), f'trained_model_all_{epoch+1}.pth')\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    if len(runs) % 2 == 1:  # 짝수 개의 요소로 만들기 위해\n",
    "        runs = np.append(runs, len(pixels))\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def compute_otsu_threshold(mask):\n",
    "    # Convert the mask to an 8-bit single-channel image\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply Otsu's method to find the optimal threshold\n",
    "    _, threshold = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    return threshold / 255  # Normalize the threshold to the range [0, 1]\n",
    "\n",
    "def evaluate_and_visualize_segmentation(model, dataloader, device):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for images, mask in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            binary_masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "            binary_masks = np.squeeze(binary_masks, axis=1)\n",
    "            binary_masks_orig = binary_masks\n",
    "\n",
    "            otsu_thresholds = [compute_otsu_threshold(mask) for mask in binary_masks_orig]\n",
    "            binary_masks = [(mask > threshold).astype(np.uint8) for mask, threshold in zip(binary_masks_orig, otsu_thresholds)]\n",
    "            \n",
    "            images_np = images.cpu().numpy()\n",
    "\n",
    "            for i in range(len(images)): # 시각화 (이미지 개수 조정)\n",
    "                mask_rle = rle_encode(binary_masks[i])\n",
    "                if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                 result.append(-1)\n",
    "                else:\n",
    "                  result.append(mask_rle)\n",
    "\n",
    "                plt.figure(figsize=(20, 5))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(np.transpose(images_np[i], (1, 2, 0)))\n",
    "                plt.title('Input Image')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(binary_masks[i], cmap='gray')\n",
    "                plt.title('Predicted Mask')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                histogram, bins = np.histogram(255 * binary_masks_orig[i, 0][0:], bins=256, range=[0, 256])\n",
    "                plt.plot(histogram, color='black')\n",
    "                plt.title('Histogram of Predicted Mask')\n",
    "                plt.xlim([0, 256])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load('trained_model_all_1.pth'))\n",
    "# # model.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "infer_dataset = ImageDataset(image_folder='test_img', mask_folder=None, transform=transform, infer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# model.to(device)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m result \u001b[39m=\u001b[39m evaluate_and_visualize_segmentation(model, infer_dataloader, device)\n",
      "Cell \u001b[1;32mIn[180], line 27\u001b[0m, in \u001b[0;36mevaluate_and_visualize_segmentation\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     28\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m         outputs \u001b[39m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\heps\\.conda\\envs\\ecg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\heps\\.conda\\envs\\ecg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\heps\\.conda\\envs\\ecg\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\heps\\.conda\\envs\\ecg\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[176], line 26\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     25\u001b[0m     image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_folder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames[index])\n\u001b[1;32m---> 26\u001b[0m     mask_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_folder, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_filenames[index])\n\u001b[0;32m     28\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "infer_dataloader = DataLoader(infer_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "result = evaluate_and_visualize_segmentation(model, infer_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submit = pd.read_csv('./submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "\n",
    "submit.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landmark1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
